{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent & Linear Regression w/ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a linear regression model to predict crop yields based off some features in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-06-02-17-31-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to determine yield\n",
    "\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "\n",
    "b1, b2 are a bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the learning part of linear regression is to figure out a set of weights w11, w12,... w23, b1 & b2 using the training data, to make accurate predictions for new data. The learned weights will be used to predict the yields for apples and oranges in a new region using the average temperature, rainfall, and humidity for that region.\n",
    "\n",
    "We'll train our model by adjusting the weights slightly many times to make better predictions, using an optimization technique called gradient descent. Let's begin by importing Numpy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3034,  0.5033,  0.3379],\n",
       "         [-0.2687,  2.0168,  0.0073]], requires_grad=True),\n",
       " tensor([-0.6933,  1.0025], requires_grad=True))"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the weights and biases, initially random values\n",
    "\n",
    "#randn is used to randomly select elements from a normal dist, centered at 0 w/ SD 1\n",
    "weights = torch.randn(2, 3, requires_grad=True)\n",
    "biases = torch.randn(2, requires_grad=True)\n",
    "\n",
    "weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our model as simply a function for now:\n",
    "\n",
    "![](2022-06-02-17-47-29.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication is done using the @ symbol w/ pytorch\n",
    "def model(x):\n",
    "    return x @ weights.t() + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 25.4078, 116.8274],\n",
      "        [ 37.6114, 154.4967],\n",
      "        [ 59.9475, 248.3006],\n",
      "        [  2.5035,  60.5889],\n",
      "        [ 50.3397, 176.5856]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "#Here each row represents the prediction for the yield of apples and orange respectively, where each row represents a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3168.0337, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How far every prediction is from the target\n",
    "diff = preds - targets\n",
    "\n",
    "#Mean Squared Error\n",
    "torch.sum(diff * diff) / diff.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, targets):\n",
    "    diff = preds - targets\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3168.0337, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On average, each element in the prediction differs by the actual target by the sqrt(mse)\n",
    "#Loss indicates how bad the model is at predicting the target values\n",
    "loss = mse(preds, targets)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss is a function of weights and biases so we can compute the gradient of each tensor\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3388.2715, -3934.9287, -2384.9937],\n",
       "        [ 4939.4468,  5967.5938,  3405.7173]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each element in the grad matrix represents the derivative of the loss w.r.t. each element of the matrix\n",
    "weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust weights and biases to reduce the loss\n",
    "The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's slope w.r.t. the weights and biases.\n",
    "\n",
    "If a gradient element is positive:\n",
    "\n",
    "increasing the weight element's value slightly will increase the loss\n",
    "decreasing the weight element's value slightly will decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-06-02-20-48-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the loss w.r.t. each element indicates the slope of the loss function w.r.t. each respective element\n",
    "\n",
    "We compute the gradient to find the direction of greatest descent in order find the minimum loss\n",
    "\n",
    "We want to iteratively adjust the value of the weights to move in the direction of greatest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use .no_grad() when you don't want to compute the gradient of the computation you're performing\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * 1e-5 # <- We call this the learning rate\n",
    "    biases -= biases.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2695,  0.5426,  0.3618],\n",
       "         [-0.3181,  1.9571, -0.0268]], requires_grad=True),\n",
       " tensor([-0.6928,  1.0019], requires_grad=True))"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2218.5178, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "weights.grad.zero_()\n",
    "biases.grad.zero_()\n",
    "print(weights.grad)\n",
    "print(biases.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's train our model using gradient descent following these steps:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "6. Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 31.5436, 107.7583],\n",
      "        [ 45.6843, 142.5701],\n",
      "        [ 69.5518, 234.0308],\n",
      "        [  8.5344,  51.7239],\n",
      "        [ 58.1251, 165.0639]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#1. Generate Prediction\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2218.5178, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#2. Calculate the loss\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2754.1799, -3251.8872, -1963.8596],\n",
      "        [ 4001.8320,  4956.2720,  2782.6272]])\n",
      "tensor([-33.5122,  48.2294])\n"
     ]
    }
   ],
   "source": [
    "#3. Compute the gradient\n",
    "\n",
    "loss.backward()\n",
    "print(weights.grad)\n",
    "print(biases.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Adjust weights by subtracting a small portion of the gradient\n",
    "\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * 1e-5\n",
    "    biases -= biases.grad * 1e-5\n",
    "    \n",
    "    #5. Reset the gradients back to zero\n",
    "    weights.grad.zero_()\n",
    "    biases.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    global weights, biases\n",
    "    for i in range(epochs):\n",
    "        # print(weights)\n",
    "        preds = model(inputs)\n",
    "        loss = mse(preds, targets)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * 1e-5\n",
    "            biases -= biases.grad * 1e-5\n",
    "            weights.grad.zero_()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6582, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "\n",
    "loss\n",
    "\n",
    "#We see now that the loss has been quite minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"danielcufino/02-gradient-descent\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/danielcufino/02-gradient-descent\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/danielcufino/02-gradient-descent'"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = '02-gradient-descent', filename='2-gradient_descent_and_lin_reg.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression using PyTorch builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We break the dataset into batches and perform gradient descent on those batches to train our model faster\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "#TensorDataset objects allow us to access input and target rows as tuples \n",
    "\n",
    "#This gives us the first three rows of inputs and first 3 rows of targets\n",
    "train_ds[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define a data loader to split our dataset into batches of a defined size\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True) #Setting shuffle to true shuffle all the data before putting it into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  87.,  65.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [101.,  44.,  37.],\n",
      "        [103.,  43.,  36.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 73.,  66.,  44.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 68.,  97.,  70.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 74.,  66.,  43.],\n",
      "        [ 92.,  87.,  64.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 80., 102.],\n",
      "        [119., 133.],\n",
      "        [ 21.,  38.],\n",
      "        [ 20.,  38.],\n",
      "        [ 22.,  37.],\n",
      "        [ 57.,  69.],\n",
      "        [ 56.,  70.],\n",
      "        [118., 134.],\n",
      "        [118., 132.],\n",
      "        [104., 118.],\n",
      "        [102., 120.],\n",
      "        [ 81., 101.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "#Iterating over a DataLoader gives you a batch of input and its corresponding batch of output \n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1585,  0.5506, -0.0265],\n",
      "        [ 0.1283, -0.0863,  0.0453]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0463,  0.2311], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Creating the model with PyTorch\n",
    "\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1585,  0.5506, -0.0265],\n",
       "         [ 0.1283, -0.0863,  0.0453]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0463,  0.2311], requires_grad=True)]"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47.2755,  5.7656],\n",
       "        [61.1351,  7.2147],\n",
       "        [85.9889,  2.4595],\n",
       "        [38.8153, 11.2871],\n",
       "        [61.8947,  3.9725],\n",
       "        [46.8834,  5.9803],\n",
       "        [60.5580,  7.3463],\n",
       "        [86.1209,  2.6331],\n",
       "        [39.2074, 11.0725],\n",
       "        [61.7097,  3.8895],\n",
       "        [46.6984,  5.8972],\n",
       "        [60.7429,  7.4294],\n",
       "        [86.5660,  2.3279],\n",
       "        [39.0003, 11.3702],\n",
       "        [62.2868,  3.7579]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate predictions\n",
    "\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4733.7285, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our optimizer to minimize loss instead of manually performing gradient descent\n",
    "#SGD Stochastic Gradient Descent: elements in batches are selected randomly\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5) #we pass model.parameters() as an argument so the optimizer knows which matrices to modify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Define a function that puts all the parts together\n",
    "\n",
    "def fit(epochs, model, loss_fn, optimizer, train_dl):\n",
    "\n",
    "    #Repeat epoch number of times\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train using the batches of data\n",
    "        for xb, yb in train_dl:\n",
    "            #1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            #3. Compute Gradients\n",
    "            loss.backward()\n",
    "            #4. Update parameters  \n",
    "            optimizer.step()\n",
    "            #5. Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Print the loss after a few epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print('Epoch [{} / {}]: Loss: {:.4f}'.format(epoch + 1, epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10 / 1000]: Loss: 548.1976\n",
      "Epoch [20 / 1000]: Loss: 381.3996\n",
      "Epoch [30 / 1000]: Loss: 337.1642\n",
      "Epoch [40 / 1000]: Loss: 300.1525\n",
      "Epoch [50 / 1000]: Loss: 267.5632\n",
      "Epoch [60 / 1000]: Loss: 238.8285\n",
      "Epoch [70 / 1000]: Loss: 213.4854\n",
      "Epoch [80 / 1000]: Loss: 191.1270\n",
      "Epoch [90 / 1000]: Loss: 171.3957\n",
      "Epoch [100 / 1000]: Loss: 153.9766\n",
      "Epoch [110 / 1000]: Loss: 138.5928\n",
      "Epoch [120 / 1000]: Loss: 125.0008\n",
      "Epoch [130 / 1000]: Loss: 112.9859\n",
      "Epoch [140 / 1000]: Loss: 102.3597\n",
      "Epoch [150 / 1000]: Loss: 92.9563\n",
      "Epoch [160 / 1000]: Loss: 84.6297\n",
      "Epoch [170 / 1000]: Loss: 77.2513\n",
      "Epoch [180 / 1000]: Loss: 70.7082\n",
      "Epoch [190 / 1000]: Loss: 64.9009\n",
      "Epoch [200 / 1000]: Loss: 59.7419\n",
      "Epoch [210 / 1000]: Loss: 55.1542\n",
      "Epoch [220 / 1000]: Loss: 51.0700\n",
      "Epoch [230 / 1000]: Loss: 47.4298\n",
      "Epoch [240 / 1000]: Loss: 44.1810\n",
      "Epoch [250 / 1000]: Loss: 41.2774\n",
      "Epoch [260 / 1000]: Loss: 38.6785\n",
      "Epoch [270 / 1000]: Loss: 36.3484\n",
      "Epoch [280 / 1000]: Loss: 34.2557\n",
      "Epoch [290 / 1000]: Loss: 32.3726\n",
      "Epoch [300 / 1000]: Loss: 30.6748\n",
      "Epoch [310 / 1000]: Loss: 29.1408\n",
      "Epoch [320 / 1000]: Loss: 27.7516\n",
      "Epoch [330 / 1000]: Loss: 26.4907\n",
      "Epoch [340 / 1000]: Loss: 25.3433\n",
      "Epoch [350 / 1000]: Loss: 24.2965\n",
      "Epoch [360 / 1000]: Loss: 23.3391\n",
      "Epoch [370 / 1000]: Loss: 22.4608\n",
      "Epoch [380 / 1000]: Loss: 21.6530\n",
      "Epoch [390 / 1000]: Loss: 20.9077\n",
      "Epoch [400 / 1000]: Loss: 20.2183\n",
      "Epoch [410 / 1000]: Loss: 19.5786\n",
      "Epoch [420 / 1000]: Loss: 18.9832\n",
      "Epoch [430 / 1000]: Loss: 18.4276\n",
      "Epoch [440 / 1000]: Loss: 17.9075\n",
      "Epoch [450 / 1000]: Loss: 17.4193\n",
      "Epoch [460 / 1000]: Loss: 16.9597\n",
      "Epoch [470 / 1000]: Loss: 16.5259\n",
      "Epoch [480 / 1000]: Loss: 16.1153\n",
      "Epoch [490 / 1000]: Loss: 15.7258\n",
      "Epoch [500 / 1000]: Loss: 15.3553\n",
      "Epoch [510 / 1000]: Loss: 15.0022\n",
      "Epoch [520 / 1000]: Loss: 14.6647\n",
      "Epoch [530 / 1000]: Loss: 14.3417\n",
      "Epoch [540 / 1000]: Loss: 14.0318\n",
      "Epoch [550 / 1000]: Loss: 13.7340\n",
      "Epoch [560 / 1000]: Loss: 13.4474\n",
      "Epoch [570 / 1000]: Loss: 13.1709\n",
      "Epoch [580 / 1000]: Loss: 12.9040\n",
      "Epoch [590 / 1000]: Loss: 12.6459\n",
      "Epoch [600 / 1000]: Loss: 12.3961\n",
      "Epoch [610 / 1000]: Loss: 12.1539\n",
      "Epoch [620 / 1000]: Loss: 11.9189\n",
      "Epoch [630 / 1000]: Loss: 11.6906\n",
      "Epoch [640 / 1000]: Loss: 11.4687\n",
      "Epoch [650 / 1000]: Loss: 11.2528\n",
      "Epoch [660 / 1000]: Loss: 11.0426\n",
      "Epoch [670 / 1000]: Loss: 10.8379\n",
      "Epoch [680 / 1000]: Loss: 10.6382\n",
      "Epoch [690 / 1000]: Loss: 10.4434\n",
      "Epoch [700 / 1000]: Loss: 10.2534\n",
      "Epoch [710 / 1000]: Loss: 10.0678\n",
      "Epoch [720 / 1000]: Loss: 9.8865\n",
      "Epoch [730 / 1000]: Loss: 9.7094\n",
      "Epoch [740 / 1000]: Loss: 9.5363\n",
      "Epoch [750 / 1000]: Loss: 9.3670\n",
      "Epoch [760 / 1000]: Loss: 9.2014\n",
      "Epoch [770 / 1000]: Loss: 9.0394\n",
      "Epoch [780 / 1000]: Loss: 8.8809\n",
      "Epoch [790 / 1000]: Loss: 8.7258\n",
      "Epoch [800 / 1000]: Loss: 8.5739\n",
      "Epoch [810 / 1000]: Loss: 8.4252\n",
      "Epoch [820 / 1000]: Loss: 8.2796\n",
      "Epoch [830 / 1000]: Loss: 8.1371\n",
      "Epoch [840 / 1000]: Loss: 7.9974\n",
      "Epoch [850 / 1000]: Loss: 7.8607\n",
      "Epoch [860 / 1000]: Loss: 7.7267\n",
      "Epoch [870 / 1000]: Loss: 7.5954\n",
      "Epoch [880 / 1000]: Loss: 7.4667\n",
      "Epoch [890 / 1000]: Loss: 7.3407\n",
      "Epoch [900 / 1000]: Loss: 7.2172\n",
      "Epoch [910 / 1000]: Loss: 7.0962\n",
      "Epoch [920 / 1000]: Loss: 6.9776\n",
      "Epoch [930 / 1000]: Loss: 6.8613\n",
      "Epoch [940 / 1000]: Loss: 6.7474\n",
      "Epoch [950 / 1000]: Loss: 6.6357\n",
      "Epoch [960 / 1000]: Loss: 6.5262\n",
      "Epoch [970 / 1000]: Loss: 6.4189\n",
      "Epoch [980 / 1000]: Loss: 6.3137\n",
      "Epoch [990 / 1000]: Loss: 6.2106\n",
      "Epoch [1000 / 1000]: Loss: 6.1095\n"
     ]
    }
   ],
   "source": [
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.1649,  70.6139],\n",
       "        [ 80.3567,  99.1806],\n",
       "        [121.8035, 135.6339],\n",
       "        [ 21.8111,  38.5679],\n",
       "        [ 98.3461, 115.7023],\n",
       "        [ 55.8828,  69.5062],\n",
       "        [ 79.8967,  99.0237],\n",
       "        [121.9257, 136.0959],\n",
       "        [ 23.0932,  39.6756],\n",
       "        [ 99.1682, 116.6531],\n",
       "        [ 56.7049,  70.4570],\n",
       "        [ 79.0746,  98.0729],\n",
       "        [122.2635, 135.7908],\n",
       "        [ 20.9890,  37.6171],\n",
       "        [ 99.6282, 116.8100]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [118., 134.],\n",
      "        [ 20.,  38.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1649,  0.6139],\n",
      "        [-0.6433, -1.8194],\n",
      "        [ 2.8035,  2.6339],\n",
      "        [-0.1889,  1.5679],\n",
      "        [-4.6539, -3.2977],\n",
      "        [-1.1172,  0.5062],\n",
      "        [-0.1033, -2.9763],\n",
      "        [ 3.9257,  4.0959],\n",
      "        [ 2.0932,  1.6756],\n",
      "        [-4.8318, -1.3469],\n",
      "        [-0.2951,  1.4570],\n",
      "        [-2.9254, -1.9271],\n",
      "        [ 4.2635,  1.7908],\n",
      "        [ 0.9890, -0.3829],\n",
      "        [-2.3718, -3.1900]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Difference in the predicted and actual targets\n",
    "print(preds - targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53.2084, 67.3782], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicted Crop yields of Temperature, Rainfall, and \n",
    "model(torch.tensor([75, 63, 44.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"danielcufino/02-gradient-descent\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/danielcufino/02-gradient-descent\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/danielcufino/02-gradient-descent'"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project = '02-gradient-descent', filename='2-gradient_descent_and_lin_reg.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some resources for learning more about linear regression and gradient descent:\n",
    "\n",
    "An visual & animated explanation of gradient descent: https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "\n",
    "For a more detailed explanation of derivates and gradient descent, see these notes from a Udacity course.\n",
    "\n",
    "For an animated visualization of how linear regression works, see this post.\n",
    "\n",
    "For a more mathematical treatment of matrix calculus, linear regression and gradient descent, you should check out Andrew Ng's excellent course notes from CS229 at Stanford University.\n",
    "\n",
    "To practice and test your skills, you can participate in the Boston Housing Price Prediction competition on Kaggle, a website that hosts data science competitions.\n",
    "\n",
    "With this, we complete our discussion of linear regression in PyTorch, and we’re ready to move on to the next topic: Working with Images & Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f77669fab35ec92083fcc4dd571d0f9418951f956a8a5354f6600963454a15c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
